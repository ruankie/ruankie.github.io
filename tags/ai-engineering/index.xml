<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI Engineering on Ruan Pretorius</title>
    <link>https://ruankie.github.io/tags/ai-engineering/</link>
    <description>Recent content in AI Engineering on Ruan Pretorius</description>
    <generator>Hugo 0.125.2</generator>
    <language>en</language>
    <lastBuildDate>Thu, 03 Oct 2024 12:00:00 +0200</lastBuildDate>
    <atom:link href="https://ruankie.github.io/tags/ai-engineering/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>PyConZA: Monitoring and Evaluating LLM Apps with Langfuse</title>
      <link>https://ruankie.github.io/publications/pyconza-24/</link>
      <pubDate>Thu, 03 Oct 2024 12:00:00 +0200</pubDate>
      <guid>https://ruankie.github.io/publications/pyconza-24/</guid>
      <description>Links ðŸ“¦ Repository ðŸ“Š Slides Abstract In the rapidly evolving landscape of AI, the ability to quickly iterate on prototypes and ensure robust performance of apps built around language models is crucial for developers and data scientists. This talk focuses on leveraging free and open-source tools to build, monitor, and evaluate LLM applications, providing a cost-effective approach to rapid development and deployment within the Python ecosystem.&#xA;Topics Covered Setting up local LLMs for fast prototyping Using Ollama for easy setup and deployment on local machines Running LLMs locally to build and test prototypes without incurring costs Monitoring LLM apps with Langfuse Introduction to Langfuse, an open-source LLM engineering platform Configuring Langfuse for tracing and evaluation Implementing Langfuse&amp;rsquo;s Python decorator and SDK for detailed monitoring LLM-assisted evaluation Using Langfuse to set up evaluation datasets and scoring responses of your LLM app Target Audience and Takeaways This talk is aimed at developers and data scientists who are interested in monitoring and optimising their LLM applications.</description>
    </item>
  </channel>
</rss>
